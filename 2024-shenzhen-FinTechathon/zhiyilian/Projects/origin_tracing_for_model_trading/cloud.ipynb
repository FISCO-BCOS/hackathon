{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fa956afdcf42e1807e19a06bfb0618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import os \n",
    "import scipy\n",
    "import torch.nn as nn\n",
    "from modified_stable_diffusion import ModifiedStableDiffusionPipeline\n",
    "import PIL\n",
    "from PIL import Image, ImageFilter,ImageEnhance\n",
    "import commpy.utilities as util\n",
    "import cv2\n",
    "from diffusers_ori.models import AutoencoderKL\n",
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import os \n",
    "import scipy\n",
    "import torch.nn as nn\n",
    "from modified_stable_diffusion import ModifiedStableDiffusionPipeline\n",
    "import PIL\n",
    "from PIL import Image, ImageFilter,ImageEnhance\n",
    "import commpy.utilities as util\n",
    "import cv2\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='diffusion watermark')\n",
    "    parser.add_argument('--w_seed', default=0, type=int)\n",
    "    # parser.add_argument('--dataset', default='Gustavosta/Stable-Diffusion-Prompts')\n",
    "    parser.add_argument('--dataset', default='coco')\n",
    "    # parser.add_argument('--dataset', default='stablediffusionDB')\n",
    "    # parser.add_argument('--model_path', default='../stable-diffusion-2-1-base')\n",
    "    parser.add_argument('--model_path', default='../stable-diffusion-v1-4')\n",
    "    parser.add_argument('--image_length', default=512, type=int)\n",
    "    parser.add_argument('--secret_length', default=48, type=int)\n",
    "    parser.add_argument('--num_inference_steps', default=25, type=int)\n",
    "    parser.add_argument('--guidancescale', default=5, type=float)\n",
    "    parser.add_argument('--reverse_inference_steps', default=25, type=int)\n",
    "    # parser.add_argument('--model', default='./encoder_decoder_pretrain/model48bit.pth', type=str)\n",
    "    # parser.add_argument('--model', default='./model48bit_finetuned.pth', type=str)\n",
    "    parser.add_argument('--model', default='./model48bit_finetuned_backup.pth', type=str)\n",
    "    args =parser.parse_known_args()[0]\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    torch.set_printoptions(sci_mode=False,profile='full')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    maxlength=150\n",
    "    \n",
    "# dataset\n",
    "dataset, prompt_key = get_dataset(args)\n",
    "dataset=promptdataset(dataset,prompt_key)\n",
    "vae = AutoencoderKL.from_pretrained('../stable-diffusion-v1-4/vae', torch_dtype=torch.float16).to(device)\n",
    "#model\n",
    "scheduler = DPMSolverMultistepScheduler.from_pretrained(args.model_path, subfolder='scheduler')\n",
    "pipe = ModifiedStableDiffusionPipeline.from_pretrained(\n",
    "        args.model_path,\n",
    "        scheduler=scheduler,\n",
    "        torch_dtype=torch.float16,\n",
    "        revision='fp16',\n",
    "        )\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "#diffusetrace\n",
    "from encoder_decoder_pretrain.watermark_model import *\n",
    "encoder=Watermark(secret_length=args.secret_length).to(device)\n",
    "if args.model !=None:\n",
    "    encoder.load_state_dict(torch.load(args.model))\n",
    "encoder.eval()\n",
    "\n",
    "def showlatent(latents,channel):\n",
    "      matrix = np.array(latents[0, channel].detach().cpu())\n",
    "      fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "      im1=axs.imshow(matrix, cmap='viridis', interpolation='nearest')\n",
    "      axs.axis('off')\n",
    "      fig.colorbar(im1).remove()\n",
    "      fig.tight_layout(pad=0, h_pad=0,rect=(0.1, 0.1, 0.9, 0.9))\n",
    "      canvas = fig.canvas\n",
    "      canvas.draw()\n",
    "      w, h = fig.canvas.get_width_height()\n",
    "      buf = np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8)\n",
    "      buf.shape = (w, h, 4)\n",
    "      buf = np.roll(buf, 3, axis=2)\n",
    "      pil_image = Image.frombytes(\"RGBA\", (w, h), buf.tobytes())\n",
    "      plt.close()\n",
    "      return pil_image\n",
    "\n",
    "def Watermark_Generation():\n",
    "    bina = torch.Tensor(args.secret).unsqueeze(-1).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "    bina = bina.expand(-1,-1,64,64)\n",
    "    matrix1,mean,logvar=encoder(bina)\n",
    "    mean=mean.reshape(-1,4,64,64)\n",
    "    logvar=logvar.reshape(-1,4,64,64)\n",
    "    eps = torch.randn_like(logvar)\n",
    "    std = torch.exp(logvar / 2)\n",
    "    matrix = eps * std + mean\n",
    "    init_latents=matrix.half()\n",
    "    args.init_latents=init_latents\n",
    "    rad=[]\n",
    "    for i in range(4):\n",
    "        rad.append(showlatent(random_latents,i))\n",
    "    return rad\n",
    "\n",
    "def Image_Generation(prompt):\n",
    "    with torch.no_grad():\n",
    "                        height,height = 512,512\n",
    "                        do_classifier_free_guidance = args.guidancescale > 1.0\n",
    "                        text_embeddings,negative_prompt_embeds = pipe.encode_prompt(\n",
    "                            prompt, device, 1, do_classifier_free_guidance\n",
    "                        )\n",
    "                        if do_classifier_free_guidance:\n",
    "                            text_embeddings = torch.cat([negative_prompt_embeds, text_embeddings])\n",
    "                        pipe.scheduler.set_timesteps(args.num_inference_steps, device=device)\n",
    "                        timesteps = pipe.scheduler.timesteps\n",
    "                        latents=args.init_latents\n",
    "                        lat=[]\n",
    "                        for i, t in enumerate(timesteps):\n",
    "                                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "                                latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "                                noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[0]\n",
    "                                if do_classifier_free_guidance:\n",
    "                                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                                    noise_pred = noise_pred_uncond + args.guidancescale * (noise_pred_text - noise_pred_uncond)\n",
    "                                latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "                                tmp=(latents / 0.18215).to(next(iter(pipe.vae.post_quant_conv.parameters())).dtype)\n",
    "                                tmp=vae.decode(tmp, return_dict=False)[0].to(device)\n",
    "                                lat.append(pipe.image_processor.postprocess(tmp.detach(), output_type='pil')[0] )\n",
    "                        latents = latents / 0.18215     \n",
    "                        latents = latents.to(next(iter(pipe.vae.post_quant_conv.parameters())).dtype)\n",
    "                        \n",
    "                        lat_fine = vae.decode(latents, return_dict=False)[0].to(device)\n",
    "                        img_fine = pipe.image_processor.postprocess(lat_fine.detach(), output_type='pil')[0] \n",
    "                        args.image=img_fine\n",
    "    return img_fine,lat\n",
    "    \n",
    "def prompt_gen(input):\n",
    "    return input\n",
    "\n",
    "def Watermark_Generation():\n",
    "    bina = torch.Tensor(args.secret).unsqueeze(-1).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "    bina = bina.expand(-1,-1,64,64)\n",
    "    matrix1,mean,logvar=encoder(bina)\n",
    "    mean=mean.reshape(-1,4,64,64)\n",
    "    logvar=logvar.reshape(-1,4,64,64)\n",
    "    eps = torch.randn_like(logvar)\n",
    "    std = torch.exp(logvar / 2)\n",
    "    matrix = eps * std + mean\n",
    "    init_latents=matrix.half()\n",
    "    args.init_latents=init_latents\n",
    "    ret=[]\n",
    "    rad=[]\n",
    "    for i in range(4):\n",
    "        ret.append(showlatent(init_latents,i))\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "def ID_gen():\n",
    "    args.secret=torch.Tensor(np.random.choice([0, 1], size=(args.secret_length))).to(device)\n",
    "    return args.secret.int().tolist()\n",
    "\n",
    "def detect():\n",
    "    reverse_latents=reverse(args.image,pipe,args).float()\n",
    "    reverse_latents = reverse_latents.view(1, -1)\n",
    "    x = encoder.decoder_projection(reverse_latents)\n",
    "    x = torch.reshape(x, (-1, *encoder.decoder_input_chw))\n",
    "    secret_tmp = torch.Tensor(args.secret).unsqueeze(-1).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "    secret_tmp = secret_tmp.expand(-1,-1,64,64)\n",
    "    average_tensor1 = torch.mean(secret_tmp, dim=(-2, -1))\n",
    "    average_tensor2 = torch.round(torch.mean(encoder.decoder(x), dim=(-2, -1)))\n",
    "    biterror=torch.sum(abs(average_tensor1-average_tensor2))\n",
    "    bitacc=1-biterror/48\n",
    "    return average_tensor2.int().tolist(),bitacc\n",
    "\n",
    "def Image_Attack(\n",
    "                  jpeg_ratio,\n",
    "                  scale_ratio,\n",
    "                  gauss_blur,\n",
    "                  gauss_noise,\n",
    "                  brightness,\n",
    "                  contrast,\n",
    "                  saturation_factor,\n",
    "                  hue_factor,\n",
    "                  \n",
    "):\n",
    "    img1=args.image\n",
    "\n",
    "\n",
    "    if jpeg_ratio!=100:\n",
    "        os.makedirs('tmp', exist_ok=True)\n",
    "        img1.save(f\"./tmp/tmp_{jpeg_ratio}_img1.jpg\", quality=jpeg_ratio)\n",
    "        img1 = Image.open(f\"./tmp/tmp_{jpeg_ratio}_img1.jpg\")\n",
    "\n",
    "   \n",
    "    if scale_ratio!=1:\n",
    "        set_random_seed(attack_seed)        \n",
    "        #获取原始图像的宽度和高度\n",
    "        width, height = img1.size\n",
    "        new_width = width * scale_ratio\n",
    "        new_height = height \n",
    "        img1 = img1.resize((int(new_width), int(new_height)), PIL.Image.BICUBIC)\n",
    "        img2 = img2.resize((int(new_width), int(new_height)), PIL.Image.BICUBIC)\n",
    "\n",
    "    if gauss_blur!=0:\n",
    "        img1 = img1.filter(ImageFilter.GaussianBlur(radius=gauss_blur))\n",
    "        img2 = img2.filter(ImageFilter.GaussianBlur(radius=gauss_blur))\n",
    "        \n",
    "    if gauss_noise!=0:\n",
    "        g_noise = (np.random.normal(0, gauss_noise, np.array(img1).shape) * 255).astype(np.uint8)\n",
    "        img1 = Image.fromarray(np.clip(np.array(img1, dtype=np.int16) + g_noise, 0, 255).astype(np.uint8))\n",
    "        \n",
    "    if brightness!=0:\n",
    "        img1 = transforms.ColorJitter(brightness=brightness)(img1)\n",
    "        img2 = transforms.ColorJitter(brightness=brightness)(img2)\n",
    "        \n",
    "    if contrast!=1:\n",
    "       img1 = transforms.ColorJitter(contrast=[contrast,contrast])(img1)\n",
    "       img2 = transforms.ColorJitter(contrast=[contrast,contrast])(img2) \n",
    "       \n",
    "    if saturation_factor!=1:\n",
    "       img1 = transforms.ColorJitter(saturation=[saturation_factor,saturation_factor])(img1)\n",
    "       img2 = transforms.ColorJitter(saturation=[saturation_factor,saturation_factor])(img2)\n",
    "       \n",
    "    if hue_factor!=0:\n",
    "       img1 = transforms.ColorJitter(hue=[hue_factor,hue_factor])(img1)\n",
    "       img2 = transforms.ColorJitter(hue=[hue_factor,hue_factor])(img2)\n",
    "       \n",
    "    args.attacked_wm_image=img1\n",
    "    return args.image, args.attacked_wm_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://62992e200c3d3a2027.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "block = gr.Blocks().queue()\n",
    "with block:\n",
    "    \n",
    "        with gr.Row():\n",
    "           gr.Markdown(\"云服务场景下的扩散模型版权保护与用户溯源\")\n",
    "        with gr.Column():\n",
    "                user=gr.Textbox(value='', label=\"用户ID(分发式场景下为特定用户微调模型组件)\")  \n",
    "        with gr.Row():       \n",
    "                ID_button = gr.Button(value=\"随机生成一个48bit的用户ID\")        \n",
    "        with gr.Row():\n",
    "           with gr.Column():\n",
    "               Watermark_latent = gr.Gallery(label=\"带水印的初始隐变量\", show_label=True,\n",
    "                    columns=[2], rows=[2], object_fit=\"contain\")\n",
    "               \n",
    "        with gr.Row():       \n",
    "                watermark_button = gr.Button(value=\"水印生成\")\n",
    "                \n",
    "        with gr.Row():\n",
    "                with gr.Column():\n",
    "                    prompt_Input = gr.Textbox(label=\"提示词输入\") \n",
    "                with gr.Column():\n",
    "                    prompt = gr.Textbox(label=\"已经被确认的提示词\")\n",
    "        with gr.Row():\n",
    "           prompt_generation_button = gr.Button(value=\"生成提示词\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        with gr.Row():\n",
    "               Generated_Watermark_image = gr.Image(label=\"生成的图像\",show_label=True,\n",
    "                   interactive=\"True\")\n",
    "        with gr.Row():\n",
    "               Generated_images = gr.Gallery(label=\"图像生成的过程\", show_label=True,\n",
    "                    object_fit=\"contain\")\n",
    "        with gr.Row():\n",
    "                  image_generation_button = gr.Button(value=\"图像生成\")\n",
    "        \n",
    "        with gr.Column():\n",
    "               with gr.Accordion(\"Attack Settings\", open=True): \n",
    "                 jpeg_ratio = gr.Slider(\n",
    "                    label=\"JPEG Compression\", minimum=0, maximum=100, value=100, step=1)\n",
    "                 scale_ratio = gr.Slider(\n",
    "                    label=\"Scale\", minimum=0.05, maximum=20, value=1, step=0.05)\n",
    "                 brightness = gr.Slider(\n",
    "                    label=\"Brightness\", minimum=0.1, maximum=3, value=0, step=0.1)\n",
    "                 contrast = gr.Slider(\n",
    "                    label=\"Contrast\", minimum=0.1, maximum=2, value=1, step=0.1)\n",
    "                 saturation_factor = gr.Slider(\n",
    "                    label=\"Saturation_factor\", minimum=0.1, maximum=2, value=1, step=0.1)\n",
    "                 hue_factor = gr.Slider(\n",
    "                    label=\"Hue_factor\", minimum=-0.5, maximum=0.5, value=0, step=0.001)\n",
    "                 gauss_blur = gr.Slider(\n",
    "                    label=\"Gauss Blur\", minimum=0, maximum=10, value=0, step=0.1)\n",
    "                 gauss_noise = gr.Slider(\n",
    "                    label=\"Gauss Noise Std\", minimum=0, maximum=0.05, value=0, step=0.0001)\n",
    "        with gr.Row():\n",
    "               ori = gr.Image(label=\"生成的图像\",show_label=True,\n",
    "                   interactive=\"True\")\n",
    "               atta = gr.Image(label=\"攻击后的\",show_label=True,\n",
    "                   interactive=\"True\")\n",
    "        with gr.Row():\n",
    "                 attack_button = gr.Button(value=\"图像处理攻击\") \n",
    "        with gr.Row():\n",
    "                with gr.Column():\n",
    "                        msg=gr.Textbox(value='', label=\"解码水印信息\") \n",
    "        with gr.Row():\n",
    "                with gr.Column():\n",
    "                        acc=gr.Textbox(value='', label=\"比特准确率\")\n",
    "        with gr.Row(): \n",
    "                  detect_button=gr.Button(value=\"水印解码\")\n",
    "        \n",
    "               \n",
    "        ID_button.click(fn=ID_gen,inputs=[],outputs=[user])\n",
    "        watermark_button.click(fn=Watermark_Generation, inputs= [],\n",
    "                               outputs=[Watermark_latent])\n",
    "        prompt_generation_button.click(fn=prompt_gen,inputs=[prompt_Input],outputs=[prompt])\n",
    "        image_generation_button.click(fn=Image_Generation, inputs= [prompt],\\\n",
    "                        outputs=[Generated_Watermark_image,Generated_images])\n",
    "        attack_button.click(fn=Image_Attack, inputs= [jpeg_ratio,\n",
    "                        scale_ratio,gauss_blur,gauss_noise,brightness,contrast,\n",
    "                        saturation_factor,hue_factor],outputs=[ori,atta])\n",
    "        \n",
    "        detect_button.click(fn=detect,inputs=[],outputs=[msg,acc])\n",
    "        \n",
    "        \n",
    "block.launch(share=True)   \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
