{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_229684/682822630.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(args.model))\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import os \n",
    "import scipy\n",
    "import torch.nn as nn\n",
    "from modified_stable_diffusion import ModifiedStableDiffusionPipeline\n",
    "import PIL\n",
    "from PIL import Image, ImageFilter,ImageEnhance\n",
    "import commpy.utilities as util\n",
    "import cv2\n",
    "from diffusers.models import AutoencoderKL\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='diffusion watermark')\n",
    "    parser.add_argument('--w_seed', default=0, type=int)\n",
    "    # parser.add_argument('--dataset', default='Gustavosta/Stable-Diffusion-Prompts')\n",
    "    parser.add_argument('--dataset', default='coco')\n",
    "    # parser.add_argument('--dataset', default='stablediffusionDB')\n",
    "    # parser.add_argument('--model_path', default='../stable-diffusion-2-1-base')\n",
    "    parser.add_argument('--model_path', default='../stable-diffusion-v1-4')\n",
    "    parser.add_argument('--image_length', default=512, type=int)\n",
    "    parser.add_argument('--secret_length', default=48, type=int)\n",
    "    parser.add_argument('--num_inference_steps', default=25, type=int)\n",
    "    parser.add_argument('--guidancescale', default=5, type=float)\n",
    "    parser.add_argument('--reverse_inference_steps', default=25, type=int)\n",
    "    # parser.add_argument('--model', default='./encoder_decoder_pretrain/model48bit.pth', type=str)\n",
    "    # parser.add_argument('--model', default='./model48bit_finetuned.pth', type=str)\n",
    "    parser.add_argument('--model', default='./model48bit_finetuned_backup.pth', type=str)\n",
    "    args =parser.parse_known_args()[0]\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    torch.set_printoptions(sci_mode=False,profile='full')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    maxlength=150\n",
    "    \n",
    "# dataset\n",
    "dataset, prompt_key = get_dataset(args)\n",
    "dataset=promptdataset(dataset,prompt_key)\n",
    "new_vae=torch.load('./disti.pth').to(device).to(torch.float32)\n",
    "vae = AutoencoderKL.from_pretrained('../stable-diffusion-v1-4/vae', torch_dtype=torch.float16).to(device)\n",
    "#model\n",
    "scheduler = DPMSolverMultistepScheduler.from_pretrained(args.model_path, subfolder='scheduler')\n",
    "pipe = ModifiedStableDiffusionPipeline.from_pretrained(\n",
    "        args.model_path,\n",
    "        scheduler=scheduler,\n",
    "        torch_dtype=torch.float16,\n",
    "        revision='fp16',\n",
    "        )\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "#diffusetrace\n",
    "from encoder_decoder_pretrain.watermark_model import *\n",
    "encoder=Watermark(secret_length=args.secret_length).to(device)\n",
    "if args.model !=None:\n",
    "    encoder.load_state_dict(torch.load(args.model))\n",
    "encoder.eval()\n",
    "\n",
    "secret= torch.ones(48, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showlatent(latents,channel):\n",
    "      matrix = np.array(latents[0, channel].detach().cpu())\n",
    "      fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "      im1=axs.imshow(matrix, cmap='viridis', interpolation='nearest')\n",
    "      axs.axis('off')\n",
    "      fig.colorbar(im1).remove()\n",
    "      fig.tight_layout(pad=0, h_pad=0,rect=(0.1, 0.1, 0.9, 0.9))\n",
    "      canvas = fig.canvas\n",
    "      canvas.draw()\n",
    "      w, h = fig.canvas.get_width_height()\n",
    "      buf = np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8)\n",
    "      buf.shape = (w, h, 4)\n",
    "      buf = np.roll(buf, 3, axis=2)\n",
    "      pil_image = Image.frombytes(\"RGBA\", (w, h), buf.tobytes())\n",
    "      plt.close()\n",
    "      return pil_image\n",
    "\n",
    "def Watermark_Generation(seed):\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    random_latents=get_random_latents(pipe,args,generator=generator)\n",
    "    args.random_latents=random_latents\n",
    "    bina = torch.Tensor(secret).unsqueeze(-1).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "    bina = bina.expand(-1,-1,64,64)\n",
    "    matrix1,mean,logvar=encoder(bina)\n",
    "    mean=mean.reshape(-1,4,64,64)\n",
    "    logvar=logvar.reshape(-1,4,64,64)\n",
    "    eps = torch.randn_like(logvar)\n",
    "    std = torch.exp(logvar / 2)\n",
    "    matrix = eps * std + mean\n",
    "    init_latents=matrix.half()\n",
    "    args.init_latents=init_latents\n",
    "    ret=[]\n",
    "    rad=[]\n",
    "    for i in range(4):\n",
    "        rad.append(showlatent(random_latents,i))\n",
    "        ret.append(showlatent(init_latents,i))\n",
    "    return ret,rad\n",
    "\n",
    "def Image_Generation(prompt):\n",
    "    with torch.no_grad():\n",
    "                        height,height = 512,512\n",
    "                        do_classifier_free_guidance = args.guidancescale > 1.0\n",
    "                        text_embeddings,negative_prompt_embeds = pipe.encode_prompt(\n",
    "                            prompt, device, 1, do_classifier_free_guidance\n",
    "                        )\n",
    "                        if do_classifier_free_guidance:\n",
    "                            text_embeddings = torch.cat([negative_prompt_embeds, text_embeddings])\n",
    "                        pipe.scheduler.set_timesteps(args.num_inference_steps, device=device)\n",
    "                        timesteps = pipe.scheduler.timesteps\n",
    "                        latents=args.init_latents\n",
    "                        lat=[]\n",
    "                        for i, t in enumerate(timesteps):\n",
    "                                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "                                latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "                                noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[0]\n",
    "                                if do_classifier_free_guidance:\n",
    "                                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                                    noise_pred = noise_pred_uncond + args.guidancescale * (noise_pred_text - noise_pred_uncond)\n",
    "                                latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "                                tmp=(latents / 0.18215).to(next(iter(pipe.vae.post_quant_conv.parameters())).dtype)\n",
    "                                tmp=new_vae.decode(tmp.float(),args.init_latents.float(), return_dict=False)[0].to(device)\n",
    "                                lat.append(pipe.image_processor.postprocess(tmp.float().detach(), output_type='pil')[0] )\n",
    "                        latents = latents / 0.18215     \n",
    "                        latents = latents.to(next(iter(pipe.vae.post_quant_conv.parameters())).dtype)\n",
    "                        \n",
    "                        lat_fine = new_vae.decode(latents.float(),args.init_latents.float(), return_dict=False)[0].to(device)\n",
    "                        img_fine = pipe.image_processor.postprocess(lat_fine.float().detach(), output_type='pil')[0] \n",
    "    return img_fine,lat\n",
    "    \n",
    "def prompt_gen(input):\n",
    "    return input\n",
    "\n",
    "def Watermark_Generation(seed):\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    random_latents=get_random_latents(pipe,args,generator=generator)\n",
    "    args.random_latents=random_latents\n",
    "    bina = torch.Tensor(secret).unsqueeze(-1).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "    bina = bina.expand(-1,-1,64,64)\n",
    "    matrix1,mean,logvar=encoder(bina)\n",
    "    mean=mean.reshape(-1,4,64,64)\n",
    "    logvar=logvar.reshape(-1,4,64,64)\n",
    "    eps = torch.randn_like(logvar)\n",
    "    std = torch.exp(logvar / 2)\n",
    "    matrix = eps * std + mean\n",
    "    init_latents=matrix.half()\n",
    "    args.init_latents=init_latents\n",
    "    ret=[]\n",
    "    rad=[]\n",
    "    for i in range(4):\n",
    "        rad.append(showlatent(random_latents,i))\n",
    "        ret.append(showlatent(init_latents,i))\n",
    "    return ret,rad\n",
    "\n",
    "def unauth_Image_Generation(prompt):\n",
    "    with torch.no_grad():\n",
    "                        height,height = 512,512\n",
    "                        do_classifier_free_guidance = args.guidancescale > 1.0\n",
    "                        text_embeddings,negative_prompt_embeds = pipe.encode_prompt(\n",
    "                            prompt, device, 1, do_classifier_free_guidance\n",
    "                        )\n",
    "                        if do_classifier_free_guidance:\n",
    "                            text_embeddings = torch.cat([negative_prompt_embeds, text_embeddings])\n",
    "                        pipe.scheduler.set_timesteps(args.num_inference_steps, device=device)\n",
    "                        timesteps = pipe.scheduler.timesteps\n",
    "                        latents=args.random_latents\n",
    "                        lat=[]\n",
    "                        for i, t in enumerate(timesteps):\n",
    "                                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "                                latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "                                noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[0]\n",
    "                                if do_classifier_free_guidance:\n",
    "                                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                                    noise_pred = noise_pred_uncond + args.guidancescale * (noise_pred_text - noise_pred_uncond)\n",
    "                                latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "                                tmp=(latents / 0.18215).to(next(iter(pipe.vae.post_quant_conv.parameters())).dtype)\n",
    "                                tmp=new_vae.decode(tmp.float(),args.random_latents.float(), return_dict=False)[0].to(device)\n",
    "                                lat.append(pipe.image_processor.postprocess(tmp.float().detach(), output_type='pil')[0] )\n",
    "                        latents = latents / 0.18215     \n",
    "                        latents = latents.to(next(iter(pipe.vae.post_quant_conv.parameters())).dtype)\n",
    "                        \n",
    "                        lat_fine = new_vae.decode(latents.float(),args.random_latents.float(), return_dict=False)[0].to(device)\n",
    "                        img_fine = pipe.image_processor.postprocess(lat_fine.float().detach(), output_type='pil')[0] \n",
    "    return img_fine,lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "IMPORTANT: You are using gradio version 4.32.2, however version 5.0.1 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://0df5253d112cfd8e25.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0df5253d112cfd8e25.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "block = gr.Blocks().queue()\n",
    "with block:\n",
    "        with gr.Row():\n",
    "           gr.Markdown(\"分发式场景下的扩散模型版权保护与用户溯源\")\n",
    "        with gr.Column():\n",
    "                user=gr.Textbox(value=secret.int().tolist(), label=\"用户ID(分发式场景下为特定用户微调模型组件)\")  \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "              with gr.Accordion(\"Watermark Generation (Random)\", open=True):\n",
    "                    watermark_seed=gr.Slider(\n",
    "                        label=\"Watermark_Seed\",\n",
    "                        minimum=0,\n",
    "                        maximum=2147483647,\n",
    "                        step=1,\n",
    "                        randomize=True,\n",
    "                    )\n",
    "        with gr.Row():\n",
    "           with gr.Column():\n",
    "               Original_latent = gr.Gallery(label=\"随机初始隐变量\", show_label=True,\n",
    "                    columns=[2], rows=[2], object_fit=\"contain\")\n",
    "           with gr.Column():\n",
    "               Watermark_latent = gr.Gallery(label=\"带水印的初始隐变量\", show_label=True,\n",
    "                    columns=[2], rows=[2], object_fit=\"contain\")\n",
    "               \n",
    "        with gr.Row():       \n",
    "                watermark_button = gr.Button(value=\"水印生成\")\n",
    "                \n",
    "        with gr.Row():\n",
    "                with gr.Column():\n",
    "                    prompt_Input = gr.Textbox(label=\"提示词输入\") \n",
    "                with gr.Column():\n",
    "                    prompt = gr.Textbox(label=\"已经被确认的提示词\")\n",
    "        with gr.Row():\n",
    "           prompt_generation_button = gr.Button(value=\"生成提示词\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        with gr.Row():\n",
    "               Generated_Watermark_image = gr.Image(label=\"生成的图像\",show_label=True,\n",
    "                   interactive=\"True\")\n",
    "        with gr.Row():\n",
    "               Generated_images = gr.Gallery(label=\"图像生成的过程\", show_label=True,\n",
    "                    object_fit=\"contain\")\n",
    "        with gr.Row():\n",
    "                  image_generation_button = gr.Button(value=\"图像生成\")\n",
    "                  \n",
    "                  \n",
    "                  \n",
    "        with gr.Row():\n",
    "               unauth_Generated_Watermark_image = gr.Image(label=\"生成的图像\",show_label=True,\n",
    "                   interactive=\"True\")\n",
    "        with gr.Row():\n",
    "               unauth_Generated_images = gr.Gallery(label=\"图像生成的过程\", show_label=True,\n",
    "                    object_fit=\"contain\")\n",
    "        with gr.Row():\n",
    "                  unauth_image_generation_button = gr.Button(value=\"随机隐变量图像生成（未授权用户、希望逃避安全机制的用户）\")\n",
    "                  \n",
    "        watermark_button.click(fn=Watermark_Generation, inputs= [watermark_seed],\n",
    "                               outputs=[Original_latent,Watermark_latent])\n",
    "        prompt_generation_button.click(fn=prompt_gen,inputs=[prompt_Input],outputs=[prompt])\n",
    "        image_generation_button.click(fn=Image_Generation, inputs= [prompt],\\\n",
    "                        outputs=[Generated_Watermark_image,Generated_images])\n",
    "        unauth_image_generation_button.click(fn=unauth_Image_Generation, inputs= [prompt],\\\n",
    "                        outputs=[unauth_Generated_Watermark_image,unauth_Generated_images])\n",
    "block.launch(share=True)   \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
